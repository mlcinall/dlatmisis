{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Домашнее задание: Visual Question Answering (VQA)\n",
    "\n",
    "## 1. Введение\n",
    "\n",
    "### Что такое VQA?\n",
    "\n",
    "**Visual Question Answering (VQA)** — задача, в которой модель должна ответить на текстовый вопрос об изображении. Например:\n",
    "- Изображение: фотография кота\n",
    "- Вопрос: \"Какого цвета кот?\"\n",
    "- Ответ: \"Рыжий\"\n",
    "\n",
    "Это мультимодальная задача, требующая понимания как визуальной, так и текстовой информации\n",
    "\n",
    "### Зачем нужны мультимодальные модели?\n",
    "\n",
    "Традиционные модели работают либо с изображениями, либо с текстом. Мультимодальные модели объединяют оба типа данных:\n",
    "- **Простой подход:** объединение эмбеддингов из разных моделей (ResNet + T5)\n",
    "- **Продвинутый подход:** сквозное обучение (CLIP, LLaVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 2. Подготовка окружения\n",
    "\n",
    "Установим необходимые библиотеки для работы с моделями и интерфейсами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "install",
    "outputId": "953ffa32-53d6-4021-bb68-510af5f1e4f5"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision transformers open_clip_torch gradio pillow pandas accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imports",
    "outputId": "4797857a-777c-46d4-d2c3-613e68394655"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from transformers import T5EncoderModel, T5Tokenizer, CLIPProcessor, CLIPModel\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import open_clip\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Используемое устройство: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Память: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## 3. Подготовка данных\n",
    "\n",
    "Создадим небольшой датасет для тестирования. Для простоты возьмем несколько изображений из CIFAR-10 и составим вопросы вручную\n",
    "\n",
    "### Задание 3.1: Загрузите датасет CIFAR-10\n",
    "\n",
    "**Что нужно сделать:**\n",
    "- Загрузите тестовую часть CIFAR-10 (используйте `torchvision.datasets.CIFAR10`)\n",
    "- Выберите 5-7 изображений из разных классов\n",
    "- Сохраните их в список `sample_images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "load_cifar",
    "outputId": "2cd487c8-1dc0-449c-e614-728dc9bd11f4"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "cifar_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=None)\n",
    "cifar_classes = cifar_dataset.classes\n",
    "\n",
    "target_classes = cifar_classes\n",
    "per_class = 20\n",
    "picked_idx = []\n",
    "count_by_class = {c: 0 for c in target_classes}\n",
    "for i in range(len(cifar_dataset)):\n",
    "    img, lbl = cifar_dataset[i]\n",
    "    name = cifar_classes[lbl]\n",
    "    if name in target_classes and count_by_class[name] < per_class:\n",
    "        picked_idx.append(i)\n",
    "        count_by_class[name] += 1\n",
    "    if all(count_by_class[c] >= per_class for c in target_classes):\n",
    "        break\n",
    "\n",
    "sample_images, sample_labels = [], []\n",
    "for i in picked_idx:\n",
    "    img, lbl = cifar_dataset[i]\n",
    "    sample_images.append(img)\n",
    "    sample_labels.append(cifar_classes[lbl])\n",
    "\n",
    "len(sample_images), sample_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset"
   },
   "source": [
    "### Задание 3.2: Создайте DataFrame с вопросами и ответами\n",
    "\n",
    "**Что нужно сделать:**\n",
    "- Для каждого изображения придумайте 1-2 вопроса\n",
    "- Вопросы могут быть о: цвете, типе объекта, количестве объектов, действиях\n",
    "- Создайте pandas DataFrame с колонками: `image_id`, `question`, `answer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "create_qa",
    "outputId": "ac916037-73aa-44b2-d2c9-72d8db0fa8b3"
   },
   "outputs": [],
   "source": [
    "qa_rows = {'image_id':[], 'question':[], 'answer':[]}\n",
    "for i, cls in enumerate(sample_labels):\n",
    "    qa_rows['image_id'].append(i)\n",
    "    qa_rows['question'].append(\"What object is in the image?\")\n",
    "    qa_rows['answer'].append(cls)\n",
    "    qa_rows['image_id'].append(i)\n",
    "    qa_rows['question'].append(\"Which category best describes the main object?\")\n",
    "    qa_rows['answer'].append(cls)\n",
    "\n",
    "df = pd.DataFrame(qa_rows)\n",
    "print(len(df), len(sample_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "rA7d4aj8GYN5",
    "outputId": "eea80d47-4155-4c5e-ac11-c03f2163805e"
   },
   "outputs": [],
   "source": [
    "def visualize_samples(images, df, n_samples=3):\n",
    "    k = min(n_samples, len(images))\n",
    "    fig, axes = plt.subplots(1, k, figsize=(5*k, 4))\n",
    "    if k == 1:\n",
    "        axes = [axes]\n",
    "    for idx, ax in enumerate(axes):\n",
    "        ax.imshow(images[idx])\n",
    "        ax.axis('off')\n",
    "        qs = df[df['image_id']==idx]['question'].tolist()\n",
    "        t = f\"Image {idx}\\n\" + \"\\n\".join([(\"Q: \"+q[:30]+\"...\") for q in qs])\n",
    "        ax.set_title(t, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(sample_images, df, n_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baseline"
   },
   "source": [
    "## 4. Baseline: ResNet + T5\n",
    "\n",
    "Создадим простой бейз, который:\n",
    "1. Извлекает эмбеддинги изображений через предобученный ResNet50\n",
    "2. Извлекает эмбеддинги вопросов через T5-small\n",
    "3. Объединяет их и предсказывает ответ через MLP\n",
    "\n",
    "### Задание 4.1: Извлеките эмбеддинги изображений\n",
    "\n",
    "**Что нужно сделать:**\n",
    "- Загрузите предобученный ResNet50\n",
    "- Удалите последний слой классификации (голову)\n",
    "- Извлеките эмбеддинги для всех изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "resnet_embeddings",
    "outputId": "20b4b67a-4203-4a62-c8e3-92852960739e"
   },
   "outputs": [],
   "source": [
    "class ImageEncoder:\n",
    "    def __init__(self, device=device):\n",
    "        self.device = device\n",
    "        try:\n",
    "            weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "            backbone = models.resnet50(weights=weights)\n",
    "            try:\n",
    "                self.transform = weights.transforms()\n",
    "            except Exception:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "                ])\n",
    "        except Exception:\n",
    "            backbone = models.resnet50(pretrained=True)\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "            ])\n",
    "        backbone.fc = nn.Identity()\n",
    "        self.model = backbone.to(self.device).eval()\n",
    "    @torch.no_grad()\n",
    "    def encode(self, images):\n",
    "        batch = torch.stack([self.transform(img) for img in images], dim=0).to(self.device)\n",
    "        feats = self.model(batch)\n",
    "        return feats.detach().cpu().numpy()\n",
    "\n",
    "image_encoder = ImageEncoder(device=device)\n",
    "image_embeddings = image_encoder.encode(sample_images)\n",
    "image_embeddings.shape\n",
    "\n",
    "print(f\"Размерность эмбеддингов изображений: {image_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5_embeddings"
   },
   "source": [
    "### Задание 4.2: Извлеките эмбеддинги вопросов\n",
    "\n",
    "**Что нужно сделать:**\n",
    "- Загрузите T5-small encoder и tokenizer\n",
    "- Токенизируйте все вопросы\n",
    "- Получите эмбеддинги (используйте mean pooling по последней скрытой размерности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232,
     "referenced_widgets": [
      "e345a0288d8e45a3ab0dbf5f93468ffa",
      "c9ebc25826674452b62cdc83df622b58",
      "4bebd3460557469b9726aacc21ceb2d7",
      "2ce1534d11e141848b054da4e3ae9207",
      "c05cb6773da247b3807e92e60337b256",
      "b31853eaf266438b96d4c1ca49d590ac",
      "05738fed41df43149fcb95933ac5c8e1",
      "9a492562bc244633badb78f76c1c2c36",
      "a2bd6252da164f7682a8029907b6f9ec",
      "4ae2eaf557aa400ca879b670ebe15348",
      "07f1df7127cd419984b39ca3fd5ba58d",
      "504cc93b3a844d0b991acaedf9092167",
      "89475900603a4ce19b0fc3896ac4608e",
      "bac91f5b39014724b7ce0ea8f4db1744",
      "8a013da7676e44749d461d1ff0a4cfc7",
      "9fcd6a8717034c04a34e22686cd68aee",
      "bb52d3bb840b49149593608c708690cd",
      "08750e4be9034a208effba60578118e4",
      "9fe47f3244264144af592057516d6b00",
      "8b206bae316a48edb8f5ddcc1127aa6b",
      "ce1f204a94a94946a299a889ce3f5dc5",
      "eed8f19e929e4c7b9ab6677431c26a88",
      "762efaf43ba64c7aa202874f802fde35",
      "87a16cfed17844418ee8d226d5632e40",
      "f2c64d2c95a344528d2d950e76048b06",
      "73be16566f68459da95ab639c2bdb4b0",
      "ba61bb79f20a4b75a441a09ba8aa60b4",
      "8a7a94fc89814ab38462bc60dcc75f34",
      "e6cbbda278fc4b38b2436509e66010c7",
      "c290196230be4699ad7e451ce58e3fa7",
      "bd77d171325f4246ac10acacf6af51e8",
      "3e5d0115a35842f1945edf4beafe2889",
      "0625f2096241489ea9d78a7b1ba2a3cc",
      "ceafbdff5695406bb20a6f390df1e659",
      "3e6e3d3cbd3b4d7a9a295df40c58897e",
      "24f5fd10e44145a8ad88630d621cb915",
      "31773f45f41d4bf79d11117e85ce86c9",
      "baca85c1a81e456f988b80950460844d",
      "363dc78d0af24d659affe85c9ce08f14",
      "2a5f258e95d44f8594f62f65cf287bbc",
      "c70a31a862a349f892f61fc5d86d68ed",
      "5bae3e66d8a94663a1b2533bc4de545f",
      "140e2ae613964c158c19dac50e79df57",
      "09eac690e68442c59e061d403f233c51",
      "3783e24eea7b4d07830bf14794fc9646",
      "5447d10424554485b9520e3161e441f8",
      "0b9428ab947c4646975b619ce699e44e",
      "e88579ebdb914c03bca04f34ae5c8f22",
      "7b9f6d76fbe847d58241922176d86d3b",
      "1d6678b42853486297a934645883162e",
      "b4c0450ead06470c9150bd50bbec6db7",
      "ea484e9132b5483c9a51b9005efde915",
      "bbfb1f2b7b1b471e91aa6ffc0996c5b7",
      "3a50126fe729434592af72ea4d3935f4",
      "b899f896fa484fcba4370ea9b54d44c6"
     ]
    },
    "id": "t5_encoder",
    "outputId": "c5f90372-1350-4ddd-f908-88894c7aed19"
   },
   "outputs": [],
   "source": [
    "class TextEncoder:\n",
    "    def __init__(self, model_name='t5-small', device=device):\n",
    "        self.device = device\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5EncoderModel.from_pretrained(model_name).to(self.device).eval()\n",
    "    @torch.no_grad()\n",
    "    def encode(self, texts):\n",
    "        tok = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "        out = self.model(**tok).last_hidden_state\n",
    "        mask = tok.attention_mask.unsqueeze(-1)\n",
    "        masked = out * mask\n",
    "        summed = masked.sum(dim=1)\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)\n",
    "        mean_pooled = (summed/lengths).detach().cpu().numpy()\n",
    "        return mean_pooled\n",
    "\n",
    "text_encoder = TextEncoder(device=device)\n",
    "question_embeddings = text_encoder.encode(df['question'].tolist())\n",
    "\n",
    "print(f\"Размерность эмбеддингов вопросов: {question_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlp_classifier"
   },
   "source": [
    "### Задание 4.3: Обучите MLP-классификатор\n",
    "\n",
    "**Что нужно сделать:**\n",
    "- Объедините эмбеддинги изображений и вопросов (конкатенация)\n",
    "- Создайте словарь всех уникальных ответов\n",
    "- Реализуйте простой MLP (2-3 слоя)\n",
    "- Обучите модель на нескольких эпохах\n",
    "\n",
    "**Примечание:** Из-за маленького датасета не ожидайте высокую точность. Цель — понять архитектуру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlp_model",
    "outputId": "48c00268-b3f0-43c1-8009-2392f80e3c2e"
   },
   "outputs": [],
   "source": [
    "answer_vocab = {ans: idx for idx, ans in enumerate(sorted(df['answer'].unique()))}\n",
    "idx_to_answer = {idx: ans for ans, idx in answer_vocab.items()}\n",
    "num_classes = len(answer_vocab)\n",
    "\n",
    "img_dim = image_embeddings.shape[1]\n",
    "txt_dim = question_embeddings.shape[1]\n",
    "\n",
    "X_img, X_txt, y_cls = [], [], []\n",
    "for ridx, row in df.iterrows():\n",
    "    img_id = int(row['image_id'])\n",
    "    X_img.append(image_embeddings[img_id])\n",
    "    X_txt.append(question_embeddings[ridx])\n",
    "    y_cls.append(answer_vocab[row['answer']])\n",
    "\n",
    "X_img = torch.tensor(np.array(X_img), dtype=torch.float32).to(device)\n",
    "X_txt = torch.tensor(np.array(X_txt), dtype=torch.float32).to(device)\n",
    "y_cls = torch.tensor(np.array(y_cls), dtype=torch.long).to(device)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_img_tr, X_img_te, X_txt_tr, X_txt_te, y_tr, y_te = train_test_split(\n",
    "    X_img.cpu().numpy(), X_txt.cpu().numpy(), y_cls.cpu().numpy(),\n",
    "    test_size=0.5, random_state=42, stratify=y_cls.cpu().numpy()\n",
    ")\n",
    "\n",
    "X_img_tr = torch.tensor(X_img_tr, dtype=torch.float32).to(device)\n",
    "X_img_te = torch.tensor(X_img_te, dtype=torch.float32).to(device)\n",
    "X_txt_tr = torch.tensor(X_txt_tr, dtype=torch.float32).to(device)\n",
    "X_txt_te = torch.tensor(X_txt_te, dtype=torch.float32).to(device)\n",
    "y_tr = torch.tensor(y_tr, dtype=torch.long).to(device)\n",
    "y_te = torch.tensor(y_te, dtype=torch.long).to(device)\n",
    "\n",
    "class VQAClassifier(nn.Module):\n",
    "    def __init__(self, image_dim, text_dim, num_classes, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(image_dim + text_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "    def forward(self, image_emb, text_emb):\n",
    "        x = torch.cat([image_emb, text_emb], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "model = VQAClassifier(img_dim, txt_dim, num_classes).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_te = -1.0\n",
    "patience, wait = 10, 0\n",
    "epochs = 200\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    logits = model(X_img_tr, X_txt_tr)\n",
    "    loss = criterion(logits, y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tr_acc = (logits.argmax(dim=-1) == y_tr).float().mean().item()\n",
    "        te_logits = model(X_img_te, X_txt_te)\n",
    "        te_acc = (te_logits.argmax(dim=-1) == y_te).float().mean().item()\n",
    "    if te_acc > best_te:\n",
    "        best_te = te_acc\n",
    "        best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "    if wait >= patience:\n",
    "        break\n",
    "\n",
    "model.load_state_dict({k:v.to(device) for k,v in best_state.items()})\n",
    "float(best_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baseline_inference"
   },
   "source": [
    "### Задание 4.4: Протестируйте baseline\n",
    "\n",
    "**Что нужно сделать:**\n",
    "- Выберите 2-3 примера из датасета\n",
    "- Получите предсказания модели\n",
    "- Выведите изображение, вопрос, истинный и предсказанный ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "test_baseline",
    "outputId": "a6bb5a73-e1c8-4377-91fa-1e3dd67f8d31"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_baseline(image_id, question):\n",
    "    model.eval()\n",
    "    img_emb = torch.tensor(image_encoder.encode([sample_images[image_id]]), dtype=torch.float32).to(device)\n",
    "    txt_emb = torch.tensor(text_encoder.encode([question]), dtype=torch.float32).to(device)\n",
    "    logits = model(img_emb, txt_emb)\n",
    "    pred_idx = int(logits.argmax(dim=-1).item())\n",
    "    return idx_to_answer[pred_idx]\n",
    "\n",
    "pred_baseline_samples = []\n",
    "for ridx in range(10):\n",
    "    r = df.iloc[ridx]\n",
    "    pred_baseline_samples.append((r['image_id'], r['question'], r['answer'], predict_baseline(int(r['image_id']), r['question'])))\n",
    "pred_baseline_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clip"
   },
   "source": [
    "## 5. CLIP Zero-Shot Baseline\n",
    "\n",
    "CLIP — это мультимодальная модель, обученная связывать изображения и тексты. Мы используем её для zero-shot VQA:\n",
    "1. Для каждой пары (изображение, вопрос) сформируем набор возможных ответов\n",
    "2. Составим промпты типа \"A photo of {answer}\"\n",
    "3. CLIP выберет наиболее вероятный ответ\n",
    "\n",
    "### Задание 5.1: Загрузите CLIP\n",
    "\n",
    "**Что нужно сделать:**\n",
    "- Загрузите CLIP модель (используйте `openai/clip-vit-base-patch32` через transformers)\n",
    "- Или используйте `open_clip` библиотеку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "a6350df879634575aa43262be9a409bb",
      "0540950ad0d944d5ae4f2e270d8daf41",
      "bb5fd6ed59cb44ada882cbc58d8a816d",
      "0df279b108a24cbbb45c59d810c1f9d7",
      "3a3775f2b10d46e9b0c098eff2fc1cb0",
      "15f56492e69840d4a583e8b25f3d0327",
      "17baa749b536465197e264debb445548",
      "c4fe6336e9bb4e97bb43d793bdde3d1f",
      "e8d79c076b7748cb9430178aca158f0f",
      "2486e0603be34983ad134ae2e9d41f49",
      "c92ead5963674fe1900cea7f3bff532d"
     ]
    },
    "id": "load_clip",
    "outputId": "9e48dea3-066c-4175-bc76-9d3449c77f0d"
   },
   "outputs": [],
   "source": [
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai', device=device)\n",
    "clip_model.eval()\n",
    "clip_tokenizer = open_clip.get_tokenizer('ViT-B-32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clip_inference"
   },
   "source": [
    "### Задание 5.2: Реализуйте zero-shot VQA с CLIP\n",
    "\n",
    "**Что нужно сделать:**\n",
    "- Для каждого изображения и вопроса создайте список возможных ответов (используйте answer_vocab)\n",
    "- Сформируйте промпты: \\\"Question: {question}. Answer: {answer}\\\"\n",
    "- Используйте CLIP для выбора наиболее подходящего ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clip_predict",
    "outputId": "22e976b6-7e19-4d8c-953c-c17435961933"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_clip(image, question, candidate_answers):\n",
    "    prompts = [f\"Question: {question}. Answer: {ans}\" for ans in candidate_answers]\n",
    "    text_tokens = clip_tokenizer(prompts).to(device)\n",
    "    text_features = clip_model.encode_text(text_tokens)\n",
    "    text_features = torch.nn.functional.normalize(text_features, dim=-1)\n",
    "    img_t = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "    image_features = clip_model.encode_image(img_t)\n",
    "    image_features = torch.nn.functional.normalize(image_features, dim=-1)\n",
    "    sims = image_features @ text_features.T\n",
    "    best_idx = int(sims.argmax(dim=-1).item())\n",
    "    return candidate_answers[best_idx]\n",
    "\n",
    "candidate_answers = sorted(df['answer'].unique().tolist())\n",
    "pred_clip_samples = []\n",
    "for ridx in range(10):\n",
    "    r = df.iloc[ridx]\n",
    "    pred_clip_samples.append((r['image_id'], r['question'], r['answer'], predict_clip(sample_images[int(r['image_id'])], r['question'], candidate_answers)))\n",
    "pred_clip_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llava"
   },
   "source": [
    "## 6. LLaVA Inference\n",
    "\n",
    "LLaVA (Large Language and Vision Assistant) — это большая мультимодальная модель, которая может генерировать текстовые ответы на вопросы об изображениях.\n",
    "\n",
    "**Внимание:** LLaVA-1.5-7B требует ~14GB GPU памяти. Если в Colab недостаточно памяти, используйте квантизацию (8-bit) или напишите мне про датасферу.\n",
    "\n",
    "### Задание 6.1: Загрузите LLaVA\n",
    "\n",
    "**Что нужно сделать:**\n",
    "- Загрузите модель `llava-hf/llava-1.5-7b-hf`\n",
    "- При необходимости используйте квантизацию для экономии памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392,
     "referenced_widgets": [
      "fd93eebd0df34c438caf16d1958ec157",
      "d0ed886812b847f08ad87ee286410d46",
      "0bdf133d8f61474a92b4a99c23ebc29f",
      "18ddf17188b44e2e97cd93fe5cefea54",
      "fe5ab2b98ce744dea6f681e87cd753db",
      "9853bbede6ea4bba83a14f947e00786a",
      "2db53542514b469e956b47bdb129553f",
      "fc1b0cd034824940886be9533dffce8f",
      "26b7021c8e304509b0f9eb71ae902f26",
      "eb810f252a8b404bb4cb9ab84cd09bcb",
      "c2030f07b7b24e7f9691bc28398a60cc",
      "bbc95806a73e43c28b62161a2471a955",
      "2f2e3aa9dfcc4b93ba81f9a2db54d4e1",
      "c7a0ecec7d1043dea720d46460c699e3",
      "2630fc09d600401eb62fb09b8d9bf426",
      "a5ec471c11d64b91a6668fbe1d9eefb0",
      "fdee06d0828449cbbab88a12ce8e217d",
      "c75b577a951e4e22a3faea8c0ed2e56c",
      "4f7f79eb457341f68479d9f318550b56",
      "3c7dd131c2654699992d0e7efc67ed63",
      "a34a1a59f1a04bd8af2118c8d1bb2751",
      "ddeee50d69f34ebfb78b50970860f4b9",
      "0f43db720fe440b0a67ede504a81e36e",
      "ab5df1cb0d18414780f5373ccf843fdd",
      "a87191d5554b4c0bb22ce2804815035c",
      "a41036c585bf429cab2cefce33cea771",
      "400d335766b84f319cacee3f03bd24d4",
      "79db7d73c5034cf9be3b34be55f58521",
      "791b0c1ca96a45319e14aa2c718e3f87",
      "df11b163a8e7457387c5419ab53316ed",
      "2866534881d24e888b7b844952ee0e8a",
      "1993056cf6184e59a762ca460313b732",
      "674b3ee797524fe9867b402a524d1145",
      "d97add8230ca49e7823daa0a9748081f",
      "dcebe9dda0924accbc3f115c3c4aad35",
      "9cff40dac7204859a27901caea8a7522",
      "a2ed033a03f84e21bf059399f9e20a64",
      "3e6d07b5282244d1a8518849bfd4a759",
      "839dd8a6d34f46fbbfe3406294d804e4",
      "7d055290cd94407f9d8cb4aafade4122",
      "cb1582d46e064599a93d9d6104741b77",
      "38811a3ca1504a18b15acc03c172dc61",
      "b6ebab88aa444608923e4134c9638939",
      "5d49483212874b56b47daa6069181642",
      "543e46d9519240aea7fff4418e97e437",
      "553faa79c32d4305a4e2829989923ccc",
      "2affa624b9b34135b7de61c78ad50b9f",
      "eff1b8a9d70349f89330d9481db766e2",
      "79caf5e8d4ae42dc90beef6da1b3c787",
      "0f9f3e30cc6d4713834ca29b4c02e75e",
      "eb4db5f266de4da5aaf56068157be035",
      "629a084fd2a54d81a9ee837513682149",
      "3f8f51ebf2fc4a51826abae8f4bfa2d9",
      "4ba87fa601364ce79badf87a2923543b",
      "1acc70624e294f028f7836d40d0410c5",
      "165a55f5aa6a4bb0ab7fce36b9b01585",
      "8a8af433d6154ab0900f67ca2554eda0",
      "92627420313b4c9d8dc4be5bfc6ad17d",
      "c88264cc515a42bfba1cde48790c39e9",
      "d1e8875ac7ef49d48c5bac1a46e476fb",
      "061b6c488320436e8253b32b479f52b4",
      "94811ee42813412ab96455a64b71295b",
      "2b883ca134084b38858536063990a06e",
      "208ea9d8dc704890929a8758571f9e74",
      "73f8cef3993f423abba79c2439df65be",
      "e4ee3054a7144c779e2b048767e84968",
      "cbcbd1bc18ea449c9f84521b3cd82a8f",
      "e5bfcd363cb4423e8b0e5a98de63a241",
      "c8304c90719e48afb04d464f210a3489",
      "12589c397d054ce4a20a1c34bffec5a8",
      "783f3f2a38cc4d6bb8044c86b7abecf5",
      "c9b8d21b7cd844f5aecce5bc8d02bba4",
      "85600e3224254819ad888ea48d5967b2",
      "e8bb2ac6c2d14a2b838f330037cee31c",
      "906254e33e59485a95be618907dc5421",
      "a4b24e39d57e4630a12efd1bae36c511",
      "e6bcba9e5972485a9c7b5b48296fd1c3",
      "258edf0b8f49493eb1d8b2482237b3e7",
      "e37dfd94104443b8920809df29465623",
      "b1ecc5eb799e42869a4337bf950f7d60",
      "1baff842938843268af7dd64425bc73f",
      "d906b31c61d84f878e6bfdf0839a6bb7",
      "584dda1062134f5aa2384ec8f5a25749",
      "d216ec52f105445ba6c9006e84043ffb",
      "4203377fc2944a7d83b97927cd00ab73",
      "66841cdc2ace4e63bb3cb833a9ae4cd0",
      "381a699419d24cd49493a2640586e793",
      "3161a2d8ce734c29a06163ab0ce8638d",
      "9fc7bc07bed24eafb017af1319dcdb5e",
      "90c1f12adc674276ae177e1c9bd41076",
      "084a7dcd6a114019b505e134b3d960a0",
      "8e4a1788937641eaa0ef4dcf28d6bb3e",
      "28488d06ac104c7382c017ab339fc073",
      "a51959e52aaa434291d7bfb5a089f7ac",
      "cd955baac283446a84cb338b6830e4dc",
      "33cb14439b494bcba2336072f9ccdc5d",
      "4ba137ad195c45a1a467820ac107e2bb",
      "bf45b2d6749a466088fd781751035955",
      "3134764643cf430b8418016ae392d623",
      "b4f9ab757e2f4f5c97a400ac0ed12022",
      "7e3b530d3cd748d0b7ee73e2767232b1",
      "1e3e67a6c9e3483b99a9ddd83606661b",
      "07093e8c527a458f939533fad7510439",
      "71ab38d9be8c47cebbcfaa2c07f05421",
      "a8bcf1082070405a86288b8f7eb9e0d2",
      "e1d604fc031c4bb5a61b175dc8c2f1b9",
      "79ba02cfe5074ece8cc87e76f899946d",
      "941c3ed757bc48e29a95cd55f50f3755",
      "ed03fbb007894e839583f1c5dac4a1ec",
      "963d5fb0d02b4046b234db7b291534af"
     ]
    },
    "id": "load_llava",
    "outputId": "bd648880-7bf3-4a53-9abd-6841e52e9ef2"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "try:\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    llava_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        \"llava-hf/llava-1.5-7b-hf\",\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    llava_processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "    print(\"LLaVA 3B (8-bit) успешно загружена.\")\n",
    "except Exception as e:\n",
    "    llava_model = None\n",
    "    llava_processor = None\n",
    "    print(\"LLaVA не загружена:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llava_inference"
   },
   "source": [
    "### Задание 6.2: Генерация ответов с LLaVA\n",
    "\n",
    "**Что нужно сделать:**\n",
    "- Реализуйте функцию для генерации ответов\n",
    "- Используйте формат промпта: \"USER: <image>\n",
    "Question: {question}\n",
    "ASSISTANT:\"\n",
    "- Протестируйте на 2-3 примерах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llava_predict",
    "outputId": "27ce9ecf-d0e1-47fb-d3e8-5f01b56a667c"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_llava(image, question, max_new_tokens=32):\n",
    "    if llava_model is None or llava_processor is None:\n",
    "        return \"N/A\"\n",
    "    conversation = [{\"role\":\"user\",\"content\":[{\"type\":\"image\"},{\"type\":\"text\",\"text\":f\"Question: {question}\"}]}]\n",
    "    prompt = llava_processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = llava_processor(images=image, text=prompt, return_tensors=\"pt\").to(llava_model.device)\n",
    "    out_ids = llava_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    text = llava_processor.batch_decode(out_ids, skip_special_tokens=True)[0]\n",
    "    if \"ASSISTANT:\" in text:\n",
    "        text = text.split(\"ASSISTANT:\")[-1].strip()\n",
    "    return text\n",
    "\n",
    "pred_llava_samples = []\n",
    "for ridx in range(10):\n",
    "    r = df.iloc[ridx]\n",
    "    pred_llava_samples.append((r['image_id'], r['question'], r['answer'], predict_llava(sample_images[int(r['image_id'])], r['question'])))\n",
    "pred_llava_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "## 7. Сравнение результатов\n",
    "\n",
    "Теперь сравним все три подхода на одних и тех же примерах.\n",
    "\n",
    "### Задание 7.1: Соберите результаты всех моделей\n",
    "\n",
    "Что нужно сделать:\n",
    "- Для каждого примера из датасета получите предсказания от всех трёх моделей\n",
    "- Создайте сравнительную таблицу\n",
    "- Проанализируйте, где какая модель работает лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "collect_results",
    "outputId": "a49e1187-e7cf-4444-b041-062c856cdf4d"
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Image ID': [],\n",
    "    'Question': [],\n",
    "    'True Answer': [],\n",
    "    'ResNet+T5': [],\n",
    "    'CLIP': [],\n",
    "    'LLaVA': []\n",
    "}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    img = sample_images[int(row['image_id'])]\n",
    "    q = row['question']\n",
    "    true = row['answer']\n",
    "\n",
    "    baseline_pred = predict_baseline(int(row['image_id']), q)\n",
    "    clip_pred = predict_clip(img, q, sorted(df['answer'].unique().tolist()))\n",
    "    llava_pred = predict_llava(img, q)\n",
    "\n",
    "    results['Image ID'].append(row['image_id'])\n",
    "    results['Question'].append(q)\n",
    "    results['True Answer'].append(true)\n",
    "    results['ResNet+T5'].append(baseline_pred)\n",
    "    results['CLIP'].append(clip_pred)\n",
    "    results['LLaVA'].append(llava_pred)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis"
   },
   "source": [
    "### Задание 7.2: Проанализируйте результаты\n",
    "\n",
    "Что нужно сделать:\n",
    "- Посчитайте accuracy для каждой модели\n",
    "- Опишите сильные и слабые стороны каждого подхода\n",
    "- Приведите примеры, где модели ошибаются или дают разные ответы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "calculate_metrics",
    "outputId": "c8b5212d-0393-4f43-e1a4-2f7c0c91b51a"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, true_answers, substring_match=False):\n",
    "    correct = 0\n",
    "    for p, t in zip(predictions, true_answers):\n",
    "        if substring_match:\n",
    "            if isinstance(p, str) and t.lower() in p.lower():\n",
    "                correct += 1\n",
    "        else:\n",
    "            if str(p).lower().strip() == str(t).lower().strip():\n",
    "                correct += 1\n",
    "    return correct / len(true_answers)\n",
    "\n",
    "baseline_acc = calculate_accuracy(results_df['ResNet+T5'], results_df['True Answer'])\n",
    "clip_acc = calculate_accuracy(results_df['CLIP'], results_df['True Answer'])\n",
    "llava_acc = calculate_accuracy(results_df['LLaVA'], results_df['True Answer'], substring_match=True)\n",
    "\n",
    "print(\"\\nТочность моделей:\")\n",
    "print(f\"ResNet+T5: {baseline_acc:.2%}\")\n",
    "print(f\"CLIP: {clip_acc:.2%}\")\n",
    "print(f\"LLaVA: {llava_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "visualize_comparison",
    "outputId": "8e0098b2-ccd2-4c51-8d19-b0e4c0d682ce"
   },
   "outputs": [],
   "source": [
    "plt.bar(['ResNet+T5', 'CLIP', 'LLaVA'], [baseline_acc, clip_acc, llava_acc])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Comparison')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis_text"
   },
   "source": [
    "### Выводы (заполните после экспериментов):\n",
    "\n",
    "Baseline (ResNet + T5):\n",
    "- Сильные стороны: высокая точность (96.75%), стабильность, простота обучения.\n",
    "- Слабые стороны: требует обучения и не обобщает на новые данные.\n",
    "\n",
    "CLIP:\n",
    "- Сильные стороны: zero-shot режим, не требует обучения, хорошая точность (87.75%).\n",
    "- Слабые стороны: путает похожие категории, чувствителен к формулировке вопроса.\n",
    "\n",
    "LLaVA:\n",
    "- Сильные стороны: генерирует содержательные ответы, хорошо понимает контекст.\n",
    "- Слабые стороны: низкая точность (40%), ответы не совпадают текстуально с метками, высокая вычислительная нагрузка.\n",
    "\n",
    "Общие наблюдения:\n",
    "ResNet+T5 оптимален для обученных систем. CLIP — лучший zero-shot вариант. LLaVA полезна для интерпретации и описаний, но не для классификации.\n",
    "\n",
    "Мнения о домашке: норм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phXFCzBAVYYs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
